<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
	<head>
		<link href="http://gmpg.org/xfn/11" rel="profile">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta http-equiv="content-type" content="text/html; charset=utf-8">

		<!-- Metadata -->
    <title>GKE Safely Drain a Nodepool without pod disruptions</title>
	<meta name="description" content="Open source contributor and Cloud Architect. Creator of websu.io and bgdestroyer.com">
	<meta property="og:description" content="Open source contributor and Cloud Architect. Creator of websu.io and bgdestroyer.com">
	<meta property="og:title" content="GKE Safely Drain a Nodepool without pod disruptions" />
	<meta property="og:type" content="article" />
	<meta property="og:url" content="https://samos-it.com/posts/gke-safe-nodepool-drain.html" />
		<meta property="og:image" content="https://samos-it.com/images/sam-profile-pic.jpg" />

		<!-- Enable responsiveness on mobile devices-->
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

		<title>Sam Stoelinga</title>

		<!-- CSS -->
		<link href="//fonts.googleapis.com/" rel="dns-prefetch">
		<link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Abril+Fatface|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet">

		<link rel="stylesheet" href="https://samos-it.com/theme/css/poole.css" />
		<link rel="stylesheet" href="https://samos-it.com/theme/css/hyde.css" />
		<link rel="stylesheet" href="https://samos-it.com/theme/css/syntax.css" />
			<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" crossorigin="anonymous">

		<!-- Feeds -->
<link href="https://samos-it.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Sam Stoelinga Full Atom Feed" />
<link href="https://samos-it.com/feeds/k8s.atom.xml" type="application/atom+xml" rel="alternate" title="Sam Stoelinga Categories Atom Feed" />

		<!-- Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-20975967-1', 'auto');
ga('send', 'pageview');
</script>
	</head>

	<body class="theme-base-0d">
<div class="sidebar">
	<div class="container sidebar-sticky">
		<div class="sidebar-about">

			<h1>
				<a href="/">
					<img class="profile-picture" src="https://samos-it.com/images/sam-profile-pic.jpg">
					Sam Stoelinga
				</a>
			</h1>
			<p class="lead"></p>
			<p class="lead">Open source contributor and Cloud Architect. Creator of websu.io and bgdestroyer.com </p>
			<p></p>
		</div>
			<ul class="sidebar-nav">
					<li><a href="https://samos-it.com/pages/about.html">About Me</a></li>
					<li><a href="https://samos-it.com/pages/hire-me.html">Hire Me</a></li>
					<li><a href="https://samos-it.com/pages/privacy-policy.html">Privacy Policy</a></li>
			</ul>
		<nav class="sidebar-social">
					<a class="sidebar-social-item" href="https://www.linkedin.com/in/samstoelinga/" target="_blank">
						<i class="fa fa-linkedin"></i>
					</a>
					<a class="sidebar-social-item" href="http://github.com/samos123/" target="_blank">
						<i class="fa fa-github"></i>
					</a>
			<a class="sidebar-social-item" href="https://samos-it.com/feeds/all.atom.xml">
				<i class="fa fa-rss"></i>
			</a>
		</nav>
	</div>
</div>		<div class="content container">
<div class="post">
    <h1 class="post-title">GKE Safely Drain a Nodepool without pod disruptions</h1>
    <span class="post-date">
        Sat 04 March 2023
| Last updated on Sun 05 March 2023
    </span>
    <p>GKE/K8s wasn't originally designed for workloads that spin up single pods
and want those pods to stay up and running on the same node
for very time. That doesn't mean those kind of workloads
aren't running on GKE. In fact, there are large GKE ML/batch platform workloads
running in production that have these characteristics.</p>
<p>This post will show how to destroy / decommission a nodepool
that's running pods that should not be disrupted. The requirements for our
decommission of nodepool is as follows:</p>
<ul>
<li>Any node that's running a batch job pod should stay up for as long
  as the batch job is running</li>
<li>The max lifetime of batch job pod is 16 days</li>
<li>The nodepool should automatically scale back to 0 after all batch job pods
  have finished running</li>
<li>New pods should not trigger the nodepool that's being decommissioned to scale
  up again</li>
</ul>
<h2>How to safely drain the node without pod disruptions?</h2>
<p>The approach we will be taking is using the new GKE nodepool taint feature.
This feature allows us to taint all existing nodes in the nodepool and
also apply the taint on any newly added nodes.</p>
<p>Let's try this feature on the nodepool named <code>default-pool</code> in the cluster <code>test-drain</code>:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">POOL</span><span class="o">=</span>default-pool
<span class="nb">export</span><span class="w"> </span><span class="nv">CLUSTER</span><span class="o">=</span>test-np-upgrades
gcloud<span class="w"> </span>container<span class="w"> </span>nodepools<span class="w"> </span>update<span class="w"> </span><span class="nv">$POOL</span><span class="w"> </span>--cluster<span class="w"> </span><span class="nv">$CLUSTER</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--node-taints<span class="w"> </span><span class="nv">upgrade</span><span class="o">=</span>true:NoSchedule
</code></pre></div>

<p>Note: You might have to set a zone or region using <code>gcloud config set compute/region us-central1</code>.</p>
<p>You would get the following error if cluster autoscaler is enabled on the nodepool:</p>
<div class="highlight"><pre><span></span><code><span class="nv">ERROR</span>:<span class="w"> </span><span class="ss">(</span><span class="nv">gcloud</span>.<span class="nv">container</span>.<span class="nv">node</span><span class="o">-</span><span class="nv">pools</span>.<span class="nv">update</span><span class="ss">)</span><span class="w"> </span><span class="nv">ResponseError</span>:<span class="w"> </span><span class="nv">code</span><span class="o">=</span><span class="mi">400</span>,<span class="w"> </span><span class="nv">message</span><span class="o">=</span><span class="nv">Updates</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="s1">&#39;taints&#39;</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">supported</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">node</span><span class="w"> </span><span class="nv">pools</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">autoscaling</span><span class="w"> </span><span class="nv">enabled</span><span class="w"> </span><span class="ss">(</span><span class="nv">as</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">workaround</span>,<span class="w"> </span><span class="nv">consider</span><span class="w"> </span><span class="nv">temporarily</span><span class="w"> </span><span class="nv">disabling</span><span class="w"> </span><span class="nv">autoscaling</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">recreating</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">node</span><span class="w"> </span><span class="nv">pool</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">updated</span><span class="w"> </span><span class="nv">values</span>.<span class="ss">)</span>.
</code></pre></div>

<p>In our case the nodepools all have cluster autoscaler enabled since this is a ML/batch
platform. So disabling autoscaler isn't an option.</p>
<p>You might be about to give up but do not! Let's carefully read the error
message. It says as a workaround consider <strong>temporarily</strong> disabling autoscaling.</p>
<p>Let's give that a try, disable autoscaler, apply taints, enable autoscaler again.
Run the following commands:</p>
<div class="highlight"><pre><span></span><code>gcloud<span class="w"> </span>container<span class="w"> </span>node-pools<span class="w"> </span>update<span class="w"> </span><span class="nv">$POOL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cluster<span class="w"> </span><span class="nv">$CLUSTER</span><span class="w"> </span>--no-enable-autoscaling<span class="w"> </span>
gcloud<span class="w"> </span>container<span class="w"> </span>node-pools<span class="w"> </span>update<span class="w"> </span><span class="nv">$POOL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cluster<span class="w"> </span><span class="nv">$CLUSTER</span><span class="w"> </span>--node-taints<span class="w"> </span><span class="nv">upgrade</span><span class="o">=</span>true:NoSchedule
gcloud<span class="w"> </span>container<span class="w"> </span>node-pools<span class="w"> </span>update<span class="w"> </span><span class="nv">$POOL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cluster<span class="w"> </span><span class="nv">$CLUSTER</span><span class="w"> </span>--enable-autoscaling<span class="w"> </span>--max-nodes<span class="w"> </span><span class="m">1000</span><span class="w"> </span>--min-nodes<span class="w"> </span><span class="m">0</span>
</code></pre></div>

<p>The <code>--max-nodes</code> value doesn't matter because the nodepool got tainted so it will never scale up again.</p>
<p>So looks like this workaround is working well to taint all nodes in a nodepool</p>
<h2>Why not use kubectl cordon or taint?</h2>
<p>The issue with that approach is that it will cause the nodepool to scale
back up again from 0 to 1 because of cluster autoscaler. The cluster
autoscaler does not try to scale up again if the taint is set using
the GKE API. This is likely an implementation detail of how the
cluster autoscaler behaves on GKE.</p>
<h2>Verifying this meets our requirements</h2>
<p>The test scenario is as follows. Before applying the taints, 4 pods were scheduled
and running that caused 4 nodes to be scaled up. So right now I still expect those 4 nodes
to be there. </p>
<p>Verify the pods and nodes are still running:</p>
<div class="highlight"><pre><span></span><code>kubectl get pods -o wide
</code></pre></div>

<p>Returns the following:</p>
<div class="highlight"><pre><span></span><code>NAME   READY   STATUS   NODE
pod1   1/1     Running  gke-test-np-upgrades-default-pool-f9fd8128-cd6h
pod2   1/1     Running  gke-test-np-upgrades-default-pool-f9fd8128-tdsf
pod3   1/1     Running  gke-test-np-upgrades-default-pool-a6056569-c577
pod4   1/1     Running  gke-test-np-upgrades-default-pool-a6056569-dqff
</code></pre></div>

<p>So that looks good.</p>
<p>Let's try spinning up another pod that's targeting this tainted
nodepool.</p>
<p>Create a file named <code>pod5.yaml</code> with the following content:</p>
<div class="highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pod5</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">cloud.google.com/gke-nodepool</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default-pool</span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sleepy</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gcr.io/google_containers/pause-amd64:3.2</span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">      </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;2000Mi&quot;</span>
</code></pre></div>

<p>Create the pod:</p>
<div class="highlight"><pre><span></span><code>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>pod5.yaml
</code></pre></div>

<p>Check to ensure cluster autoscaler won't scale up the nodepool. Run the following:</p>
<div class="highlight"><pre><span></span><code>kubectl<span class="w"> </span>describe<span class="w"> </span>pod<span class="w"> </span>pod5
</code></pre></div>

<p>You should see this included in the output:</p>
<div class="highlight"><pre><span></span><code>Status:<span class="w"> </span>Pending
..
Warning<span class="w">  </span>FailedScheduling<span class="w">   </span>default-scheduler<span class="w">   </span><span class="m">0</span>/4<span class="w"> </span>nodes<span class="w"> </span>are<span class="w"> </span>available:<span class="w"> </span><span class="m">4</span><span class="w"> </span>node<span class="o">(</span>s<span class="o">)</span><span class="w"> </span>had<span class="w"> </span>taint<span class="w"> </span><span class="o">{</span>upgrade:<span class="w"> </span>true<span class="o">}</span>,<span class="w"> </span>that<span class="w"> </span>the<span class="w"> </span>pod<span class="w"> </span>didn<span class="s1">&#39;t tolerate.</span>
<span class="s1">Normal   NotTriggerScaleUp  cluster-autoscaler  pod didn&#39;</span>t<span class="w"> </span>trigger<span class="w"> </span>scale-up:<span class="w"> </span><span class="m">2</span><span class="w"> </span>max<span class="w"> </span>node<span class="w"> </span>group<span class="w"> </span>size<span class="w"> </span>reached
</code></pre></div>

<p>This shows that the pod is pending and not triggering a scale up event. This is
great and exactly what was desired.</p>
<p>Now, let's verify the nodepool scales down automatically all the way to 0 when all
pods are deleted.</p>
<p>Delete all the batch jobs:</p>
<div class="highlight"><pre><span></span><code>kubectl delete pod pod1 pod2 pod3 pod4
</code></pre></div>

<p>Note that in a normal situation you would wait for the pods to finish execution
instead of having to delete them. Once finished, the pod automatically is deleted
and autoscaler does the same scale down logic.</p>
<p>Wait for 10 to 15 minutes and then run:</p>
<div class="highlight"><pre><span></span><code>kubectl<span class="w"> </span>get<span class="w"> </span>nodes
</code></pre></div>

<p>In my case, there seems to be 1 node still running. You might wonder why that is?
It turns out that kube-dns and other GKE system services are still running on
one of the nodes and are preventing the cluster autoscaling from scaling down.
You can solve this by following my blog post on <a href="https://samos-it.com/posts/gke-system-services-kube-dns-dedicated-nodepool.html">moving GKE
system services to a dedicated nodepool</a>.</p>
<p>If you already moved your GKE system services then you would have seen 0 nodes left.</p>
<p>So this concludes that the GKE nodepool tainting feature is an effective way to
safely decommission nodepools without disrupting existing pods.</p>
<p>This was tested on GKE 1.21 and behavior is not guaranteed to stay the same in future releases</p>
    <!-- Ezoic - bottom_of_page - bottom_of_page -->
    <div id="ezoic-pub-ad-placeholder-104"> </div>
    <!-- End Ezoic - bottom_of_page - bottom_of_page -->
<div id="disqus_thread"></div>
<script type="text/javascript">
	var disqus_shortname = 'samosit';
	(function() {
		var d = document, s = d.createElement('script'); s.type = 'text/javascript'; s.async = true;
		s.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
		s.setAttribute('data-timestamp', +new Date());
		(d.head || d.body).appendChild(s);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>
		</div>
	</body>
</html>